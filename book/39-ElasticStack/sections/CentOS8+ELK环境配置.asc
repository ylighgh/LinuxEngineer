=== CentOS8+ELK环境配置

**CentOS 8**

==== 安装Java

[source, bash]
----
dnf install -y java-11-openjdk java-11-openjdk-devel java-11-openjdk-headless
----

==== 增加YUM源

[source, bash]
----
rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

cat << EOF > /etc/yum.repos.d/elasticsearch.repo
[elasticsearch]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=0
autorefresh=1
type=rpm-md
EOF
----

查看elasticsearch源中的包列表：

[source, console]
----
$ dnf --disablerepo="*" --enablerepo="elasticsearch" list available
Available Packages
apm-server.x86_64                            7.7.1-1                       elasticsearch
app-search.noarch                            7.6.2-1                       elasticsearch
auditbeat.x86_64                             7.7.1-1                       elasticsearch
elasticsearch.aarch64                        7.7.1-1                       elasticsearch
elasticsearch.x86_64                         7.7.1-1                       elasticsearch
enterprise-search.noarch                     7.7.1-1                       elasticsearch
filebeat.x86_64                              7.7.1-1                       elasticsearch
heartbeat-elastic.x86_64                     7.7.1-1                       elasticsearch
journalbeat.x86_64                           7.7.1-1                       elasticsearch
kibana.x86_64                                7.7.1-1                       elasticsearch
logstash.noarch                              1:7.7.1-1                     elasticsearch
metricbeat.x86_64                            7.7.1-1                       elasticsearch
packetbeat.x86_64                            7.7.1-1                       elasticsearch
----

==== Elasticsearch

===== 安装

[source, bash]
dnf --disablerepo="*" --enablerepo="elasticsearch" install -y elasticsearch

===== 配置

[source, bash]
----
sed -i '17s/#cluster.name: my-application/cluster.name: myapp/' /etc/elasticsearch/elasticsearch.yml

sed -i '23s/#node.name: node-1/node.name: node-1/' /etc/elasticsearch/elasticsearch.yml

sed -i '55s/#network.host: 192.168.0.1/network.host: 0.0.0.0/' /etc/elasticsearch/elasticsearch.yml

sed -i '68s/#discovery.seed_hosts: .*/discovery.seed_hosts: ["0.0.0.0\/0"]/' /etc/elasticsearch/elasticsearch.yml

sed -i '72s/#cluster.initial_master_nodes: .*/cluster.initial_master_nodes: ["node-1"]/' /etc/elasticsearch/elasticsearch.yml
----

[IMPORTANT]
====
使用阿里云的ECS服务器配置Elasticsearch时，需要明确指定本机内网IP地址。否则，会出现9200端口已经启动，但无法创建或查询index。

ECS配置示例：

* network.host: 172.24.109.12
* discovery.seed_hosts: ["172.24.109.12"]
====

===== 开机启动

[source, bash]
systemctl enable elasticsearch

===== 启动服务

[source, bash]
systemctl start elasticsearch

==== Kibana

===== 安装

[source, bash]
dnf --disablerepo="*" --enablerepo="elasticsearch" install -y kibana

===== 配置

[source, bash]
----
sed -i '7s/#server.host: "localhost"/server.host: "0.0.0.0"/' /etc/kibana/kibana.yml
sed -i '28s/#elasticsearch.hosts: .*/elasticsearch.hosts: ["http:\/\/0.0.0.0:9200"]/' /etc/kibana/kibana.yml
sed -i '115s/#i18n.locale: "en"/i18n.locale: "en"/' /etc/kibana/kibana.yml
----

[IMPORTANT]
====
如果Elasticsearch的 `"network.host"` 参数值为具体的IP地址，比如 `"172.24.109.12"`。

那么，Kibana中的 `"elasticsearch.hosts"` 同样需要设置为 `"172.24.109.12"`，而不能使用 `"0.0.0.0"`。
====

===== 开机启动

[source, bash]
systemctl enable kibana

===== 启动服务

[source, bash]
systemctl start kibana

===== 最后

如果系统IP为 172.24.109.12，则访问 http://172.24.109.12:5601/

==== Logstash

[red, big]#“如何将Nginx的Web日志导入Elasticsearch？”#

===== 日志处理管道

日志记录增加到Elasticsearch时，需要使用管道实时清洗、处理日志格式。

.Nginx日志格式
[source, text]
----
log_format  main  '$host $remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';
----

执行以下命令，在Elasticsearch中增加管道：

[source, bash]
----
curl -H "Content-Type: application/json" -X PUT http://0.0.0.0:9200/_ingest/pipeline/web_log -d '
{
    "description": "",
    "processors": [
        {
            "dissect": {
                "field": "message",
                "pattern": "%{domain} %{server_port} %{client_ip} - - [%{web_timestamp}] \"%{method} %{uri} HTTP/%{version}\" %{status} %{response_time} %{body_bytes} \"%{referer}\" \"%{user_agent}\" \"%{x_forwarded_for}\" %{server_ip}"
            }
        },
        {
            "grok": {
                "field": "uri",
                "on_failure": [
                    {
                "set": {
                    "field": "ext",
                    "value": ""
                }
                    },
                    {
                "set": {
                    "field": "url_base",
                    "value": "{{uri}}"
                }
                    }
                ],
                "patterns": [
                    "%{GREEDYDATA:url_base}\\.%{WORD:ext}"
                ]
            }
        },
        {
            "geoip": {
                "field": "client_ip",
                "target_field": "client_ip_geo"
            }
        },
        {
            "geoip": {
                "field": "server_ip",
                "target_field": "server_ip_geo"
            }
        },
        {
            "user_agent": {
                "field": "user_agent"
            }
        },
        {
            "date": {
                "field": "web_timestamp",
                "target_field": "@timestamp",
                "formats": [
                    "dd/MMM/yyyy:HH:mm:ss Z"
                ],
                "timezone" : "Asia/Shanghai"
            }
        },
        {
            "convert": {
                "field": "server_port",
                "type": "integer"
            }
        },
        {
            "convert": {
                "field": "status",
                "type": "integer"
            }
        },
        {
            "convert": {
                "field": "response_time",
                "type": "float"
            }
        },
        {
            "convert": {
                "field": "body_bytes",
                "type": "integer"
            }
        },
        {
            "set": {
                "field": "hour_of_day",
                "value": 12
            }
        },
        {
            "grok": {
                "field": "web_timestamp",
                "patterns": [
                    "\\d+/\\w+/\\d+:%{NUMBER:hour_of_day:int}:\\d+:\\d+ \\+\\d+"
                ]
            }
        }
    ]
}
'
----

===== 准备日志文件

日志文件存放路径:: ~/es_log/nginx_logs

[source, console]
----
[root@dell7 nginx_logs]# ls
access.log-20200501  access.log-20200512  access.log-20200523  access.log-20200603
access.log-20200502  access.log-20200513  access.log-20200524  access.log-20200604
access.log-20200503  access.log-20200514  access.log-20200525  access.log-20200605
access.log-20200504  access.log-20200515  access.log-20200526  access.log-20200606
access.log-20200505  access.log-20200516  access.log-20200527  access.log-20200607
access.log-20200506  access.log-20200517  access.log-20200528  access.log-20200608
access.log-20200507  access.log-20200518  access.log-20200529  access.log-20200609
access.log-20200508  access.log-20200519  access.log-20200530  access.log-20200610
access.log-20200509  access.log-20200520  access.log-20200531
access.log-20200510  access.log-20200521  access.log-20200601
access.log-20200511  access.log-20200522  access.log-20200602
----

===== 配置文件

[source, bash]
----
mkdir -p ~/es_log

cat << EOF > ~/es_log/logstash.conf
input{
  file {
    # https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html#plugins-inputs-file-delimiter
    # 默认以"\n"分割，文件必须至少一个换行符
    path => "${HOME}/es_log/nginx_logs/access.log*"
    start_position => "end"
    stat_interval => "1 second"
  }
}
output{
    elasticsearch {
            #这里可以是数组，可以是多个节点的地址，会自动启用负载均衡
            hosts => ["0.0.0.0:9200"]
            index => "foo_com"
            pipeline => "web_log"
    }
    #file  {
    #    path => "/var/log/logstash.log"
    #    codec => json
    #}
}
EOF
----

[IMPORTANT]
====
如果Elasticsearch的 `"network.host"` 参数值为具体的IP地址，比如 `"172.24.109.12"`。

那么，Logstash配置中的 `output.elasticsearch.hosts` 需要设置为 `["172.24.109.12:9200"]`。
====

===== 导入Nginx日志到Elasticsearch

导入数据到ES：

[source, bash]
/usr/share/logstash/bin/logstash -f ~/es_log/logstash.conf


每次运行的结果会记录到文件：

/usr/share/logstash/data/plugins/inputs/file/.sincedb_100caf6f694120ba2483577719e4a564

文件内容为：

[source, text]
----
270004749 0 2049 584450300 1592393767.626652 /root/es_log/nginx_logs/access.log-20200501
272281186 0 2049 471367497 1592394059.861428 /root/es_log/nginx_logs/access.log-20200502
----

如果想重新导入，删除 .sincedb 文件即可。

